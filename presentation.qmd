---
title: "Linear Models"
author: "George Savva (Quadram Institute Bioscience)"
echo: true
format:
  revealjs:
    theme: [dark]
    slide-number: true
    css: "custom.css"
    scrollable: true
    transition: slide
    incremental: false
  html:
    toc: true
---

# Introduction

## Session outline

-   Understanding linear models
-   Rethinking statistics in terms of LMs
-   Examples, discussion and practical work with R

## Getting our R packages

Start RStudio..

Make sure you have these packages installed..

```{r eval=FALSE}
install.packages(c("UsingR", "equatiomatic", "emmeans", "broom", 
                   "performance", "ggplot2", "gamlss", "car",
                   "patchwork", "ggbeeswarm", "readxl"))
```

```{r getsetup, warning=FALSE, message=FALSE}
library(UsingR) # dataset
library(readxl) # reading data from excel files
library(equatiomatic) # making equations from models
library(emmeans) # marginal means
library(broom) # enhancements to model
library(performance) # model diagnostics
library(ggplot2) # making graphs
library(gamlss) # enhanced modelling
library(car)    # hypothesis tests 
library(ggbeeswarm) # beeswarm graphs
library(patchwork) # place plots side-by-side

```

## Files for the session:

Data:

* `diversity.csv` 
* `survey.csv`
* `observations.csv`

Code, slides and handouts:

* `presentation.qmd`
* `presentation.html`
* `linearmodels.pptx`

## What is a linear model?

. . .

```{r echo=FALSE}

data("kid.weights") 
ggplot(kid.weights , aes(x=age,y=height))  + geom_point() + 
  theme_bw(base_size = 18)+ 
  labs(x="Age (years)",y="Height (ins)")
```

* What questions might we ask?
* What form could the answer take?

## What is a linear model?

```{r echo=FALSE}

data("kid.weights") 
ggplot(kid.weights , aes(x=age,y=height))  + geom_point() + 
  theme_bw(base_size = 18)+ 
  stat_smooth(method="lm", geom="line")+
  labs(x="Age (years)",y="Height (ins)")
```

$$\text{Height}=a+b\times \text{Age}$$

## What is a linear model

* Describes a linear relationship between an [outcome]{style="color:lightblue"} and its [predictors]{style="color:lightgreen"}.

* What is the model for?

* Who is interested and what will it be used for?

## Fitting our model in R

```{r }
model1 <- lm( height ~ age , data = kid.weights )
extract_eq(model1)
```

. . . 

```{r }
tidy(model1)
extract_eq(model1, use_coefs=TRUE)
```


## What can we do with this?

::: {.incremental}
* Describe the relationship?
* Predict new values of height from age?
:::
--- 

```{r  }

newages = data.frame(age=12*(1:20))

model1 |> predict (newdata = newages) |> cbind(newages)
```

## Prediction intervals

```{r addingpredictions}
model1 |> augment(interval="prediction") |>
  ggplot(aes(x=age,y=height))   + 
  theme_bw(base_size = 18)+ 
  geom_ribbon( aes(y=.fitted, ymin=.lower, ymax=.upper), alpha=0.2,fill="blue") + 
  geom_line( aes(y=.fitted)) + 
  geom_point() +
  labs(x="Age (months)",y="Height (ins)")

```

## Confidence intervals

```{r addingconfidence}
model1 |> augment(interval="confidence") |>
  ggplot(aes(x=age,y=height))   + 
  theme_bw(base_size = 18)+ 
  geom_ribbon( aes(y=.fitted, ymin=.lower, ymax=.upper), alpha=0.2,fill="blue") + 
  geom_line( aes(y=.fitted)) + geom_point() +
  labs(x="Age (months)",y="Height (ins)")

```
## Our model is done:

```{r }
extract_eq(model1)
extract_eq(model1, use_coefs = TRUE)
```

Standard deviation of 'error' = `r round(sigma(model1),2)`

$$\epsilon \sim N(0,4.77^2)$$

## Summary 1

::: {.incremental}
* A linear model describes the linear relationship between an outcome and predictors
* Can be used for prediction or inference

* Simple linear models are fit in R using `lm`.
* `broom::augment` makes it easy to get predictions.
* Visualisations allow us to check model fit.

* Confidence intervals reflect uncertainty around means
* Prediction intervals reflect likely variation in new points
:::

## Example 2 - Weight by Age

Can we apply the same method to this dataset?

Can you forsee a problem?

```{r }
ggplot(kid.weights, aes(x=age,y=weight))   + 
  theme_bw(base_size = 18)+ 
  geom_point() + labs(x="Age (months)",y="Weight (lbs)")
```

## Example 2

What are the limitations of the 'regular' linear model?

```{r }
model2 <- lm( weight ~ age , data = kid.weights )
model2 |> augment(interval="prediction") |>
  ggplot(aes(x=age,y=weight))+theme_bw(base_size = 18)+ 
  geom_ribbon(aes(y=.fitted, ymin=.lower, ymax=.upper),alpha=0.2,fill="blue") + 
  geom_line( aes(y=.fitted)) + geom_point() +
  labs(x="Age (months)",y="Weight (lbs)")
```


## Enhancing our model 1

We can model how the error term changes with age.

```{r message=FALSE}
model2b <- gamlss( weight ~ age , sigma.formula = ~age ,
                   data = kid.weights, control = gamlss.control(trace=F))
centiles.fan(model2b, cent=c(2.5,97.5))
points(kid.weights$age , kid.weights$weight)

```

## Enhancing our model 1

```{r }
summary(model2b)
```


## Enhancing our model 2

We can also model how the *skew* changes with age!

```{r }
model2c <- gamlss( weight ~ age , sigma.formula = ~age , nu.formula = ~age ,
            data = kid.weights , family="BCPE", control = gamlss.control(trace=F))
centiles.fan(model2c, cent=c(2.5,97.5))
points(kid.weights$age , kid.weights$weight)
```

## Enhancing our model 2

```{r }
summary(model2c)
```

## Summary 2
::: {.incremental}
* We can model things other than the mean!

* In the old days (when I was training) this would have been very difficult

* Now `gamlss` and Bayesian analysis makes it easy

* How good does a model need to be?
:::
## Learning objectives for today {.smaller}
::: {.incremental}
-   Distributions and Modelling
    -   Estimating parameters with linear models
    -   Re-framing *all* hypothesis tests as linear models
    -   Continuous and discrete predictors
    -   Interpreting and diagnosing models

-   Extensions and generalisations:
    -   Multiple predictors / interactions (more complex hypotheses)
    -   Log-transformations
    -   Logistic regression for binary data
:::
## Aim

-   Start to understand the 'modelling' approach to statistics

-   Enable you to explore this extensive framework to do almost any kind of statistical / ML analysis

-   Get familiar with how to do this in R

-   Become a statistician?

## About me!

:::{.incremental}
- Statistician at Quadram Institute Bioscience since 2017

- Support for epidemiology / genomics / ecology / clinical trials / pre-clinical research
    * Some statistical thinking goes a long way
    * Many problems would be much simpler if they were re-framed in terms of linear models.
    * Other approaches make life difficult and lead to inefficiencies and mistakes
    * Statistics is a <span style="color:red">super-power!</span>
:::
## About you!

- Name, previous work, experience with stats software.

# Inference, Models and Distributions

## A simple motivating problem {.smaller}

:::{.incremental}
-   Suppose we are interested in understanding the typical heights of UEA students?

-   How should we go about this?

-   Can you frame a question in mathematical / statistical terms.

    -   What should the answer look like?
    -   How will we know we are done?
:::
. . .

::: callout-tip
## Setting the question
How should you set the scope of a statistical problem?
:::

## Inference

* What is the *target population*?

* What is the *sampling frame*?

* What is the *sample*?

## A distribution and a model {.incremental}

-   Students heights are not all the same, they have a 'distribution'.

-   What kind of data is a 'height?'

-   There is an average (central tendency) and a variance (spread).

-   We also have to assume or determine the shape of the distribution.

-   As a baseline - assume continuous variables are **independently normally distributed**

## Model

::: columns
::: {.column width="50%"}
-   So, for example:

    "Heights are normally distributed with mean 180cm and standard deviation 10cm"

-   We can represent this visually

-   Is it likely to be good enough to describe the distribution of student heights?
:::

::: {.column width="10%"}
:::

::: {.column width="40%"}
::: {.fragment .fade-in}
```{r echo=FALSE}
#| fig-height: 10
plot.function(\(x) dnorm(x,180,10), xlim = c(120,240), xlab="Height", ylab="Density")
```
:::
:::
:::

. . .

::: callout-tip
## Models

What is a model?
:::

## Writing this model mathematically

$$\text{Height} \sim N(\text{mean} , \text{variance})$$

. . .

$$H \sim N(\mu , \sigma^2)$$

. . .

We describe how individual heights arise like this:

$$h_i = \mu + \epsilon_i\quad\text{where}\quad\epsilon_i \sim N(0,\sigma^2)$$

. . .

The model has a [structural]{style="color: green;"} part and an [error]{style="color: red;"} part

This is the simplest linear model.

## So what is a model? {.smaller}

Here's our model for heights:

$$
\boxed{h_i = \mu + \epsilon_i\quad\text{where}\quad\epsilon_i \sim N(0,\sigma^2)}
$$

:::{.incremental}
-   A model is a mathematical or statistical way of writing down how we think our outcome is distributed

-   The model needs to be good enough for the purpose!

    -   ***All models are wrong, but some are useful***

-   It needs to link the aspects of the process we are interested in (*true but unknown* mean $\mu$ and variance $\sigma^2$) to the data that we can *observe* ($h_i$).

    -   Here it's just a mean and variance we want to learn about but we can add things to this equation if we think they affect height.
:::
## Do I *need* a model to do analysis?

-   Yes! Whenever you're doing any kind of statistics there is an underlying model.

-   An implicit understanding of how your data came to be

    -   **how the data is generated** and
    -   **how it was collected.**

-   *Ad hoc* statistical procedures sometimes obscure hide these assumptions

-   But with linear modelling they are more explicit.

# 'Doing' statistics

## Doing statistics with the model

-   Our model:

. . .

$$h_i = \mu + \epsilon_i\quad\text{where}\quad\epsilon_i \sim N(0,\sigma^2)$$

-   If we estimate $\mu$ and $\epsilon$, we have describes the height distribution and we are done!

. . .

::: callout-note
## Key insight!

Once we have a model, *all* of statistical analysis is just estimating model parameters!
:::

## Estimating the model

-   We can use data to estimate the parameters

-   If we get enough $h_i$'s (at least 2!) we can estimate the values of $\mu$ and $\sigma$.

-   Several ways these models are estimated:

    -   Numerical optimisation
    -   Solving with algebra
    -   Maximum likelihood
    -   Bayesian resampling

-   (Not getting into this..the computer will do it)

::: callout-note
We can use this logic to show that the sample mean is the best estimate for the $\mu$
:::

## Estimating the model using R

Enter some data and check it's OK:

```{r echo=TRUE}
height = c(173,177,160,165,172,182,157,175,167)
dat <- data.frame(height)
head(dat)
```

Estimate the model:

```{r echo=TRUE}
model1 <- lm( height ~ 1 , data=dat)
```

--- 

Display the model summary:

```{r echo=TRUE, eval=TRUE}
summary(model1)


```

## Uncertainty {.incremental}
* Is our parameter estimate perfect? Have we completely discovered the average height of UEA students?

* The confidence interval and standard error reflect the uncertainty in the estimate.

* (There is also uncertainty in the standard deviation estimate, but this is harder to calculate using R and we don't often use it.)

## Thinking about uncertainty

What can we really say about the mean and spread of student heights?

Is it plausible that the true mean height is 170cm?

. . . 

```{r }
linearHypothesis(model1, "(Intercept)=170")
```

. . . 

Try some other values.  160cm, 163cm?

--- 

```{r }
confint(model1)

broom::tidy(model1 , conf.int=TRUE)
```

* The confidence interval gives us every *plausible* value of the parameter given the data.

* That is, every value not rejected at p<0.05 (for a 95% interval)

## Diagnostics

* How good is our model? What were the model assumptions?

 -   Heights are normally distributed
 -   Individuals heights are independent of each other
 -   (Our sample is representative)

--- 

* How can we check normality?

* A `qqplot` compares model residuals with a normal distribution:

```{r }
plot(model1 , which=2)
```

* How might we check assumptions of independence and representativeness?

## Summary 3

* Statistical inference is trying to estimate something fixed but unknown in a population using observed data in a sample
* The model links the unknown parameters to the observed data
* Allows us to do prediction and inference

* There is a duality between confidence intervals and p-values
* We can check normality using qqplots.  A visual inspection like this is better than statistical `tests` for normality that you might see.

# Adelaide data

## Exercise

A student questionnaire was conducted at Adelaide University. We'll use this dataset to answer some interesting questions.

1.  Load data into R (from `"survey.xlsx"`)

2.  Estimate a model in R to find the distribution of student heights.

3.  Check whether the heights are normally distributed

4.  Check whether using the `mean` and `sd` functions in R gives you the same answer.

# Extending our linear model

## Adding a predictor {.smaller}

:::{.incremental}
-   Let's improve our model.

-   Remind ourselves of the purpose of a model

    -   We might have a specific hypothesis to test

    -   Or we might want to predict an outcome for a new individual

    -   Or just understand the factors that contribute to our outcome in a population

-   Can we improve our height prediction?

-   Can we add a factor to our model that helps to explain height?
:::

## {.smaller}

Different average for men and women.

. . .

We might write this as:

$$
h_i=\left\{\begin{array}{ll}
\mu_\text{male} + \epsilon_i & \text{(if male)}\\
\mu_\text{female} + \epsilon_i & \text{(if female)}
\end{array}\right.\quad\text{where}\quad\epsilon_i \sim N(0,\sigma^2)
$$

. . .

But we would usually write this as:

$$
\boxed{h_i = \mu + \beta x_i+\epsilon_i \quad\text{where}\quad \left\{\begin{array}{ll}
x_i=1 &\text{if male;}\\
x_i=0 & \text{if female}
\end{array}\right.}
$$

. . .

Can you interpret this model? 

. . . 

What are the mean heights for men and women under this model? 

. . . 

What does $\beta$ represent?

## Estimating our new model using R

```{r }
dat <- read_excel("survey.xlsx", na="NA") |> remove_missing()
table(dat$Sex)
head(dat)
```

## Plotting

A graph is a good way to start modelling!

```{r }
ggplot(dat , aes(x=Sex, y=Height)) + 
  geom_boxplot() + geom_beeswarm() + theme_bw()
```

## Estimating with R

```{r }
model2 <- lm( Height ~ Sex , data=dat)
extract_eq(model2)
extract_eq(model2 , use_coefs=TRUE)

```

## Diagnostic plots

How well does our model fit its assupmtions

```{r }
par(mfrow=c(2,2))
plot(model2)
```

## Interpretation

-   Interpreting coefficients and confidence intervals

. . .

```{r }
tidy(model2, conf.int=TRUE)
extract_eq(model2, use_coefs = TRUE)
```


--- 

-   Marginal means

. . .

```{r }
emmeans(model2 , ~Sex)
```

-   Comparing models

. . .


```{r }
model1 <- lm( Height ~ 1 , data=dat)
anova(model1, model2)
```

## Summary 4

* A linear model can be used to replace a t-test

* (Comparison of a single outcome across two independent groups)

* Why should we prefer this to a t-test?

## A multiple regression

Let's repeat this process to understand how hand span depends on height.

```{r }
ggplot(data=dat, aes(Height, Wr.Hnd, shape=Sex)) + geom_point() 
```

Can you write down a model for how hand span depends on height? Can you estimate it using R?
--- 

```{r }
model3 <- lm(data=dat, Wr.Hnd ~ Height)
summary(model3)
```

Interpret and diagnose (test model assumptions) this model

## Multiple predictors

Can we better explain the hand width model by adding a second predictor?

```{r }
model4 <- lm(data=dat, Wr.Hnd ~ Height + Sex)
summary(model4)
```

------------------------------------------------------------------------

Compare the two models. What do they tell you? Which is fitting the data better?

```{r }
extract_eq(model3, use_coefs = TRUE)
extract_eq(model4, use_coefs = TRUE)

sigma(model3)
sigma(model4)

```

--- 

Is the new model *significantly* better?

```{r }
anova(model4, model3)
```

## Predictions

```{r }
(ggplot(data=augment(model3), aes(Height, Wr.Hnd)) + geom_point() + geom_line(aes(y=.fitted), lwd=1))+
(ggplot(data=augment(model4), aes(Height, Wr.Hnd, color=Sex)) + geom_point() + geom_line(aes(y=.fitted), lwd=1) +scale_color_manual(values=c("black", "red")))
```

## Interactions {.smaller}

Finally, we might ask if there's any evidence that the slope is different between men and women.

How could we make a model to test this hypothesis?

. . .

```{r }
model5 <- lm(data=dat, Wr.Hnd ~ Height + Sex + Height:Sex)
model5 <- lm(data=dat, Wr.Hnd ~ Height * Sex)
extract_eq(model5)
```

---

```{r }
summary(model5)
```

---

```{r }
extract_eq(model4, use_coefs = TRUE)
extract_eq(model5, use_coefs = TRUE, wrap=TRUE, terms_per_line = 2)
```


------------------------------------------------------------------------

```{r }
(ggplot(data=augment(model3), aes(Height, Wr.Hnd)) + geom_point() + geom_line(aes(y=.fitted), lwd=1))+
(ggplot(data=augment(model4), aes(Height, Wr.Hnd, color=Sex)) + geom_point() + geom_line(aes(y=.fitted), lwd=1) +scale_color_manual(values=c("black", "red"))) + 
(ggplot(data=augment(model5), aes(Height, Wr.Hnd, color=Sex)) + geom_point() + geom_line(aes(y=.fitted), lwd=1) +scale_color_manual(values=c("black", "red"))) +
  plot_layout(guides="collect")
```

## Model comparison {.smaller}

ANOVA compares the (1) improvement in fit between models to (2) the improvement that would be expected if there was no real difference in the population.

The p-values reflect the statistical significance of the improvement as we increase the model complexity.

```{r }
anova(model3, model4, model5)
```

Is the interaction term needed?

## Summary {.incremental}

* We can add multiple predictors to improve a model
* Linear models can replace ANCOVA
* Interactions tell us whether effects are different across sub-groups
* Models can be compared using p-values to test whether additional terms are needed

# Extensions

## Everything is a linear model {.smaller}

What statistical tests/procedures have we replaced with a linear model?

-   Unpaired T-test
-   Correlation
-   ANOVA
-   ANCOVA

All other statistical tests can be reframed in this way:

-   Models for count or categorical outcomes
-   Paired data
-   Non-parametric tests
-   More complex machine learning
-   Non-linear models are usually straightforward extensions

<https://lindeloev.github.io/tests-as-linear/#9_teaching_materials_and_a_course_outline>

## Exercise {.smaller}

Look at the diversity data in R.

```{r }
diversityDay <- read.csv("diversity.csv")
```

This is data from a real experiment to explore gut microbial diversity over time in hospital patients.

We'll use linear models to test whether `diversity` changes over `time`.

1. What is the mean and standard deviation of diversity across all samples?
2. Plot how microbial diversity varies with time.
3. Is microbial diversity *correlated* with time?
4. Is the correlation statistically significant?
5. What happens if we adjust for patient (add patient to the model)?

## Dealing with non-normal data {.incremental}

-   So far we've seen data that can be modelled using normal distributions, where the relationships are genuinely straight lines.

-   Biological data very often does not act like this

-   What options do we have?

  * Funkier models?
  * Transformations?

## Animal traits

The body mass of animals is linked to their metabolic rate.

```{r }
animals <- read.csv("observations.csv")
ggplot(animals, aes(body.mass,metabolic.rate,col=phylum)) + 
  geom_point() + facet_wrap(~phylum, scale="free")
```

Can we fit a linear model to this?

## Logarithmic transformation

The same data plotted on a different scale:

```{r }
ggplot(animals, aes(body.mass,metabolic.rate,col=phylum)) + 
  geom_point() + scale_x_log10() + scale_y_log10()
```


## Logarithmic transformations {.smaller}

Instead of modelling how $y$ depends on $x$, we can model how $\log(y)$ depends on $\log(x)$.

. . .

-   This is a very commonly used approach and it is surprising how often and how well it works.

$$\log(y_i) = a + b\log(x_i) + \epsilon_i$$

. . . 


-   Implies

$$y_i = \exp(a + b\log(x_i) + e) = e^a\times x_i^{b}\times e^{\epsilon_i}$$

-   So instead of an additive model, we have a multiplicative one.

-   But we estimate using R as a regular linear model, using $\log(y)$ as the outcome and $\log(x)$ as the predictor.

--- 

```{r }
modelAnimals <- lm(data=animals , log(metabolic.rate) ~ log(body.mass)*phylum)
summary(modelAnimals)
```

--- 


```{r }
augment(modelAnimals, interval="prediction") |>
  ggplot(aes(x=`log(body.mass)`, y=`log(metabolic.rate)`, col=phylum, fill=phylum)) +  
  geom_point() + 
  geom_line(aes(y=.fitted)) +
  geom_ribbon(aes(y=.fitted,ymin=.lower,ymax=.upper),alpha=0.2) + 
  facet_wrap(~phylum, scale="free")
  

```

--- 

```{r }
animals$predicted <- predict(modelAnimals, newdata = animals, type="response")

augment(modelAnimals, interval="prediction") |>
  ggplot(aes(x=exp(`log(body.mass)`), y=exp(`log(metabolic.rate)`), col=phylum, fill=phylum)) +  
  geom_point() + 
  geom_line(aes(y=exp(.fitted))) +
  geom_ribbon(aes(y=exp(.fitted),ymin=exp(.lower),ymax=exp(.upper)),alpha=0.2) + 
  facet_wrap(~phylum, scale="free")
  

```

--- 

## Summary 5

* Linear models can be useful for data that does not look linear!

* Next time:

* Binary data (logistic regression)

* Multilevel models (more complex errors)