---
title: "Linear Models"
author: "George Savva (Quadram Institute Bioscience)"
echo: true
format:
  revealjs:
    theme: [dark]
    slide-number: true
    scrollable: true
    transition: slide
    incremental: true
  html:
    toc: true
---

# Introduction

## Session outline

-   Understanding linear models
-   Rethinking statistics in terms of LMs
-   Examples, discussion and practical work with R

## Getting our R packages

Start RStudio..

Make sure you have these packages installed..

```{r getsetup, warning=FALSE}
library(UsingR) # dataset
library(equatiomatic) # making equations from models
library(emmeans) # marginal means
library(broom) # enhancements to model
library(performance) # model diagnostics
library(ggplot2) # making graphs
library(gamlss) # enhanced modelling
library(car)    # hypothesis tests 
library(ggbeeswarm) # beeswarm graphs

# install.packages("UsingR", "equatiomatic", "emmeans", "broom", "performance", "ggplot2", "gamlss", "car")

```

## Files for the session:

## What is a linear model?

```{r echo=FALSE}

data("kid.weights") 
ggplot(kid.weights , aes(x=age,y=height))  + geom_point() + 
  theme_bw(base_size = 18)+ 
  labs(x="Age (years)",y="Height (ins)")
```

What questions might we ask?
What form could the answer take?

## What is a linear model?

```{r echo=FALSE}

data("kid.weights") 
ggplot(kid.weights , aes(x=age,y=height))  + geom_point() + 
  theme_bw(base_size = 18)+ 
  stat_smooth(method="lm", geom="line")+
  labs(x="Age (years)",y="Height (ins)")
```

$$\text{Height}=a+b\times \text{Age}$$

## What is a linear model

* Describes the relationship between an outcome and its predictors.

* What is the model for?

## Fitting our model in R

```{r }
model1 <- lm( height ~ age , data = kid.weights )
extract_eq(model1)
tidy(model1)
extract_eq(model1, use_coefs=TRUE)
```


## What can we do with this?

* Describe the relationship?

* Predict new values of height from age?

--- 

```{r  }

newages = data.frame(age=12*(1:20))

model1 |> predict (  newdata = newages ) |> cbind(newages)
```

## Prediction intervals (variation between points)

```{r addingpredictions}
model1 |> augment(interval="prediction") |>
  ggplot(aes(x=age,y=height))   + 
  theme_bw(base_size = 18)+ 
  geom_ribbon( aes(y=.fitted, ymin=.lower, ymax=.upper), alpha=0.2,fill="blue") + 
  geom_line( aes(y=.fitted)) + 
  geom_point() +
  labs(x="Age (months)",y="Height (ins)")

```

## Confidence intervals (uncertainty in the average)

```{r addingconfidence}
model1 |> augment(interval="confidence") |>
  ggplot(aes(x=age,y=height))   + 
  theme_bw(base_size = 18)+ 
  geom_ribbon( aes(y=.fitted, ymin=.lower, ymax=.upper), alpha=0.2,fill="blue") + 
  geom_line( aes(y=.fitted)) + geom_point() +
  labs(x="Age (months)",y="Height (ins)")

```
## Our model is done:

```{r }
extract_eq(model1, use_coefs = TRUE)
```

Standard error of variance = `r round(sigma(model1),2)`


## Example 2

Weight by age.  Can we apply the same method?

Can you forsee a problem?

```{r }
ggplot(kid.weights, aes(x=age,y=weight))   + 
  theme_bw(base_size = 18)+ 
  geom_point() +
  labs(x="Age (months)",y="Weight (lbs)")
```

## Example 2

What are the limitations of the 'regular' linear model?

```{r }
model2 <- lm( weight ~ age , data = kid.weights )
model2 |> augment(interval="prediction") |>
  ggplot(aes(x=age,y=weight))   + 
  theme_bw(base_size = 18)+ 
  geom_ribbon( aes(y=.fitted, ymin=.lower, ymax=.upper), alpha=0.2,fill="blue") + 
  geom_line( aes(y=.fitted)) + geom_point() +
  labs(x="Age (months)",y="Weight (lbs)")
```


## Enhancing our model 1

We can model how the error term changes with age.

```{r message=FALSE}
model2b <- gamlss( weight ~ age , sigma.formula = ~age ,
                   data = kid.weights, control = gamlss.control(trace=F))
centiles.fan(model2b, cent=c(2.5,97.5))
points(kid.weights$age , kid.weights$weight)

```

## Enhancing out model 1

```{r }
summary(model2b)
```


## Enhancing our model 2

We can model how the skew changes with age.

```{r }
model2c <- gamlss( weight ~ age , sigma.formula = ~age , nu.formula = ~age ,
            data = kid.weights , family="BCPE", control = gamlss.control(trace=F))
centiles.fan(model2c, cent=c(2.5,97.5))
points(kid.weights$age , kid.weights$weight)
```

## Enhancing our model 2

```{r }
summary(model2c)
```


## Learning objectives

-   Distributions and Modelling
    -   Estimating parameters
    -   Re-framing hypothesis tests as linear models
    -   Continuous and discrete predictors
    -   Interpreting and diagnosing models

-   Extensions and generalisations:
    -   Multiple predictors / interactions (More complex hypotheses)
    -   Log-transformations
    -   Logistic regression for binary data

## Aim

-   Start to understand the 'modelling' approach to statistics

-   Enable you to explore this extensive framework to do almost any kind of statistical / ML analysis

-   Become a statistician?

## About me!

- Statistician at Quadram Institute Bioscience (2017-present)

- Support for epidemiology / genomics / ecology / clinical trials / pre-clinical research
    * Some statistical thinking goes a long way
    * Many problems would be much simpler if they were re-framed in terms of linear models.
    * Other approaches make life difficult and lead to inefficiencies and mistakes

## About you!

- Name, previous work, experience with stats software.

# Models and distributions

## A simple motivating problem

-   Suppose we are interested in understanding the typical heights of UEA students?

-   How should we go about this?

-   Can you frame a question in mathematical / statistical terms.

    -   What should the answer look like?
    -   How will we know we are done?

. . .

::: callout-tip
## Setting the question

How should you set about the scope of a statistical problem?
:::

## A distribution and a model

-   Students heights are not all the same, they have a 'distribution'.

-   What kind of data is a 'height?'

-   There is an average (central tendency) and a variance (spread).

-   We also have to assume or determine the shape of the distribution.

-   As a baseline - assume continuous variables are **independently normally distributed**

## Model

::: columns
::: {.column width="50%"}
-   So, for example:

    "Heights are normally distributed with mean 180cm and standard deviation 10cm"

-   We can represent this visually

-   Is it likely to be good enough to describe the distribution of student heights?
:::

::: {.column width="10%"}
:::

::: {.column width="40%"}
::: {.fragment .fade-in}
```{r echo=FALSE}
#| fig-height: 10
plot.function(\(x) dnorm(x,180,10), xlim = c(120,240), xlab="Height", ylab="Density")
```
:::
:::
:::

. . .

::: callout-tip
## Models

What is a model?
:::

## Writing this model mathematically

$$\text{Height} \sim N(\text{mean} , \text{variance})$$

. . .

$$H \sim N(\mu , \sigma^2)$$

. . .

We describe how individual heights arise like this:

$$h_i = \mu + \epsilon_i\quad\text{where}\quad\epsilon_i \sim N(0,\sigma^2)$$

. . .

The model has a [structural]{style="color: green;"} part and an [error]{style="color: red;"} part

This is the simplest linear model.

## So what is a model?

Here's our model for heights:

$$
\boxed{h_i = \mu + \epsilon_i\quad\text{where}\quad\epsilon_i \sim N(0,\sigma^2)}
$$

-   A model is a mathematical or statistical way of writing down how we think our outcome is distributed

-   The model needs to be good enough for the purpose!

    -   ***All models are wrong, but some are useful***

-   It needs to link the aspects of the process we are interested in (mean $\mu$ and variance $\sigma^2$) to the data that we can observe ($h_i$).

    -   Here it's just a mean and variance but we can add things to this equation if we think they affect height.

## Do I *need* a model to do analysis?

-   Yes! Whenever you're doing any kind of statistics there is an underlying model.

-   An implicit understanding of how your data came to be

    -   **how the data is generated** and
    -   **how it was collected.**

-   *Ad hoc* statistical procedures sometimes obscure hide these assumptions

-   But with linear modelling they are more explicit.

# 'Doing' statistics

## Doing statistics with the model

-   Our model:

. . .

$$h_i = \mu + \epsilon_i\quad\text{where}\quad\epsilon_i \sim N(0,\sigma^2)$$

-   If we estimate $\mu$ and $\epsilon$, we have describes the height distribution and we are done!

. . .

::: callout-note
## Key insight!

Once we have a model, *all* of statistical analysis is just estimating model parameters!
:::

## Estimating the model

-   We can use data to estimate the parameters

-   If we get enough $h_i$'s (at least 2!) we can estimate the values of $\mu$ and $\sigma$.

-   Several ways these models are estimated:

    -   Numerical optimisation
    -   Solving with algebra
    -   Maximum likelihood
    -   Bayesian resampling

-   (Not getting into this..the computer will do it)

::: callout-note
We can use this logic to show that the sample mean is the best estimate for the $\mu$
:::

## Estimating the model using R

Enter some data and check it's OK:

```{r echo=TRUE}
height = c(173,177,160,165,172,182,157,175,167)
dat <- data.frame(height)
head(dat)
```

Estimate the model:

```{r echo=TRUE}
model1 <- lm( height ~ 1 , data=dat)

print(model1)

```

Display the model summary:

```{r echo=TRUE, eval=TRUE}
summary(model1)


```

## Uncertainty

Is our parameter estimate perfect? Have we completely discovered the average height of UEA students?

The confidence interval and standard error reflect the uncertainty in the estimate.

There is also uncertainty in the standard deviation estimate, but this is harder to calculate using R and we don't often use it.

## Thinking about uncertainty

What can we really say about the mean and spread of student heights?

Is it plausible that the true mean height is 170cm?  160cm?

```{r }
library(car)
linearHypothesis(model1, "(Intercept)=170")
linearHypothesis(model1, "(Intercept)=160")
linearHypothesis(model1, "(Intercept)=163.5")

confint(model1)

broom::tidy(model1 , conf.int=TRUE)
```

The confidence interval gives us every plausible value of the parameter given the data.

## Diagnostics

* How good is our model? What were the model assumptions?

 -   Heights are normally distributed
 -   Individuals heights are independent of each other
 -   (Our sample is representative)

. . . 

* How we check these?

   - Why are heights normal distributed?


## Exercise

A student questionnaire was conducted at Adelaide University. We'll use this dataset to answer some interesting questions.

1.  Load data into R.

2.  Estimate a model in R to find the distribution of student heights.

3.  Check whether the heights are normally distributed

4.  Check whether using the `mean` and `sd` functions in R gives you the same answer.

# Extending our linear model

## Adding a predictor (finally!)

-   Let's improve our model.

-   Remind ourselves of the purpose of a model

    -   We might have a specific hypothesis to test

    -   Or we might want to predict an outcome for a new individual

    -   Or just understand the factors that contribute to our outcome in a population

-   Can we improve our height prediction?

-   Can we add a factor to our model that helps to explain height?

## 

Height varies with a normal distribution, but with a different average for men and women.

. . .

We might write this as:

$$
h_i=\left\{\begin{array}{ll}
\mu_\text{male} + \epsilon_i & \text{(if male)}\\
\mu_\text{female} + \epsilon_i & \text{(if female)}
\end{array}\right.\quad\text{where}\quad\epsilon_i \sim N(0,\sigma^2)
$$

. . .

But we would usually write this as:

$$
\boxed{h_i = \mu + \beta x_i+\epsilon_i \quad\text{where}\quad \left\{\begin{array}{ll}
x_i=1 &\text{if male;}\\
x_i=0 & \text{if female}
\end{array}\right.}
$$

. . .

Can you interpret this model? What are the mean heights for men and women under this model? What does $\beta$ represent?

## Estimating our new model using R

Using the Adelaide survey data:

```{r }
library(readxl)
dat <- read_excel("survey.xlsx", na="NA")
dat <- dat[!is.na(dat$Sex), ]
table(dat$Sex)
dat
```

## Plotting

Make some plots:

```{r }
library(ggbeeswarm)
ggplot(dat , aes(x=Sex, y=Height)) + geom_boxplot() + geom_beeswarm() + theme_bw()
```

## Estimating with R

```{r }
model2 <- lm( Height ~ Sex , data=dat)
summary(model2)
extract_eq(model2 , useCoefs=TRUE)
```

## Diagnostic plots

How well does our model fit its assupmtions

```{r }
par(mfrow=c(2,2))
plot(model2)
```

## Interpretation

-   Interpreting coefficients and confidence intervals

. . .

```{r }
tidy(model2, conf.int=TRUE)
extract_eq(model2, use_coefs = TRUE)
```


-   Statistical significance

. . .

```{r }
coef(summary(model2))
```

--- 

-   Marginal means

. . .

```{r }
emmeans(model2 , ~Sex)
```

-   Comparing models

. . .


```{r }
model1 <- lm( Height ~ 1 , data=dat)
anova(model1, model2)
```

## A continuous predictor

Let's repeat this process to understand how hand span depends on height.

```{r }
ggplot(data=dat, aes(Height, Wr.Hnd, shape=Sex)) + geom_point() 
```

Can you write down a model for how hand span depends on height?

--- 

```{r }
model3 <- lm(data=dat, Wr.Hnd ~ Height)
summary(model3)
```

## Multiple predictors

Can we better explain the hand width model by adding a second predictor?

```{r }
model4 <- lm(data=dat, Wr.Hnd ~ Height + Sex)
summary(model4)

extract_eq(model4)

```

------------------------------------------------------------------------

Compare the two models. What do they tell you? Which is better?

```{r }
library(modelsummary)
modelsummary(list(model3, model4))
```

## Predictions

```{r }

library(patchwork)

(ggplot(data=augment(model3), aes(Height, Wr.Hnd)) + geom_point() + geom_line(aes(y=.fitted), lwd=1))+
(ggplot(data=augment(model4), aes(Height, Wr.Hnd, color=Sex)) + geom_point() + geom_line(aes(y=.fitted), lwd=1) +scale_color_manual(values=c("black", "red")))
```

## Interactions

Finally, we might ask if there's any evidence that the slope is different between men and women.

How could we make a model to test this hypothesis?

---

```{r }
model5 <- lm(data=dat, Wr.Hnd ~ Height + Sex + Height:Sex)
model5 <- lm(data=dat, Wr.Hnd ~ Height * Sex)
extract_eq(model5)
```

---

```{r }
summary(model5)
```

------------------------------------------------------------------------

```{r }
(ggplot(data=augment(model3), aes(Height, Wr.Hnd)) + geom_point() + geom_line(aes(y=.fitted), lwd=1))+
(ggplot(data=augment(model4), aes(Height, Wr.Hnd, color=Sex)) + geom_point() + geom_line(aes(y=.fitted), lwd=1) +scale_color_manual(values=c("black", "red"))) + 
(ggplot(data=augment(model5), aes(Height, Wr.Hnd, color=Sex)) + geom_point() + geom_line(aes(y=.fitted), lwd=1) +scale_color_manual(values=c("black", "red"))) + plot_layout(guides="collect")
```

## Model comparison

ANOVA compares the improvement in fit between models to the improvement that would be expected if there was no real change.

```{r }
anova(model3, model4, model5)
```

# Extensions

## Everything is a linear model

What statistical tests/procedures have we replaced with a linear model?

-   Unpaired T-test
-   Correlation
-   ANOVA
-   ANCOVA

All other statistical tests can be reframed in this way:

-   Models for count or categorical outcomes
-   Paired data
-   Non-parametric tests
-   More complex machine learning
-   Non-linear models are usually straightforward extensions

https://lindeloev.github.io/tests-as-linear/#9_teaching_materials_and_a_course_outline  

## Exercise

Look at the iris data in R.

We'll use linear models to answer some questions about the relationship between the petals and the sepals.

1. What is the mean and standard deviation of sepal length?
2. Does sepal length vary by species?
3. Is sepal length predicted by sepal width?
4. What happens if you stratify by species?

## Dealing with non-normal data

-   So far we've seen data that can be modelled using normal distributions.

-   Biological data very often does not act like this

-   What options do we have?

## Animal traits

```{r }
animals <- read.csv("observations.csv")
library(ggplot2)
ggplot(animals, aes(body.mass,metabolic.rate,col=phylum)) + geom_point() + scale_x_log10() + scale_y_log10() + facet_wrap(~phylum)

```

## Logarithmic transformations

Instead of modelling on y, we can model log(y).

-   This is a very commonly used approach and it is surprising how often and how well it works.

$$\log(y_i) = a + bx_i + \epsilon_i$$

-   Implies

$$y_i = \exp(a + bx + e) = e^a\times e^{bx_i}\times e^{\epsilon_i}$$

or

$$y_i = \alpha\times \beta^{x_i}\times e^{\epsilon_i}$$

-   So instead of an additive model, we have a multiplicative one.

-   But we estimate it as a regular linear model, using $\log(y)$ as the outcome.
